# Layer dimensions
mlp_input: 512
mlp_tag_hidden: 16
mlp_arc_hidden: 512
mlp_lab_hidden: 128
# Lexers
lexers:
  words:
    embedding_size: 128
    word_dropout: 0.6
  chars:
    embedding_size: 64
    lstm_output_size: 128
  fasttext: {}
# Training hyperparameters
encoder_dropout: 0.6
mlp_dropout: 0.5
freeze_fasttext: false
batch_size: 32
epochs: 128
lr:
  base: 0.001