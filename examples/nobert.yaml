# Layer dimensions
word_embedding_size: 128
charlstm_output_size: 128
char_embedding_size: 64
freeze_fasttext: false
mlp_input: 512
mlp_tag_hidden: 16
mlp_arc_hidden: 512
mlp_lab_hidden: 128
# Training hyperparameters
encoder_dropout: 0.6
mlp_dropout: 0.5
word_dropout: 0.6
batch_size: 32
epochs: 128
lr: 0.001
# Word embeddings
lexer: "default"
