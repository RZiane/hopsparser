# Layer dimensions
mlp_input: 1024
mlp_tag_hidden: 16
mlp_arc_hidden: 512
mlp_lab_hidden: 128
# Training hyperparameters
encoder_dropout: 0.5
mlp_dropout: 0.5
word_dropout: 0.5
batch_size: 8
epochs: 2
lr:
  base: 0.00003
  schedule:
    shape: linear
    warmup_steps: 100
lexers:
  words:
    embedding_size: 256
  bert:
    model: "lgrobol/roberta-minuscule"
    layers: "*"
    subwords_reduction: "mean"
  chars:
    embedding_size: 64
    lstm_output_size: 128
  fasttext: {}
